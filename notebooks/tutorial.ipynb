{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "noted-afghanistan",
   "metadata": {},
   "source": [
    "# AllenNLP fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "large-consolidation",
   "metadata": {},
   "source": [
    "AllenNLP is a great open-source deep learning research library, paritcularly famous for NLP. This notebook serves as a tutorial to go over fundamental concepts of ALlenNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-aquarium",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "neural-intake",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[I, do, n't, hate, notebooks, ,, I, just, do, n't, like, them, .]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from allennlp.data.tokenizers import Token, Tokenizer, SpacyTokenizer, WhitespaceTokenizer, CharacterTokenizer\n",
    "\n",
    "text = \"I don't hate notebooks, I just don't like them.\"\n",
    "\n",
    "tokenizer = SpacyTokenizer()\n",
    "\n",
    "tokens = tokenizer.tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-jason",
   "metadata": {},
   "source": [
    "# Token Indexers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-problem",
   "metadata": {},
   "source": [
    "A Token Indexer turns tokens into indices or list of indices. We won't be able to see how they operate until slightly later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sustainable-layout",
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.token_indexers import SingleIdTokenIndexer, TokenCharactersIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "continuing-repository",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_indexer = SingleIdTokenIndexer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-species",
   "metadata": {},
   "source": [
    "# Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-production",
   "metadata": {},
   "source": [
    "Training exampels are represented as `Instances`, each containing typed `Fields`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "detailed-berry",
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.fields import TextField, LabelField"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-tissue",
   "metadata": {},
   "source": [
    "A `text field` is for storing text, and also needs one or more `TokenIndexers` that will be used to convert the text into indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "optional-harbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field = TextField(tokens, {'tokens': token_indexer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "alternate-sharp",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field._indexed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "opened-denmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_field = LabelField('technology')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-patrick",
   "metadata": {},
   "source": [
    "# Instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-tobago",
   "metadata": {},
   "source": [
    "Each instance is made up of fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "amended-stick",
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.instance import Instance\n",
    "instance = Instance({'text': text_field, 'category': label_field})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfactory-alexandria",
   "metadata": {},
   "source": [
    "# Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-cosmetic",
   "metadata": {},
   "source": [
    "Based on our instances we construct a `Vocabulary` which contains the various mappings token <-> index, label <-> index and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cordless-tsunami",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "494e9456feb44232a80fee956ae682e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='building vocab', max=1.0, style=ProgressStyle(descriptionâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from allennlp.data.vocabulary import Vocabulary\n",
    "\n",
    "vocab = Vocabulary.from_instances([instance])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-intelligence",
   "metadata": {},
   "source": [
    "Here you can see that our vocabulary has two mappings, a `tokens` mappings and `labels` mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "arabic-scheduling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_TokenToIndexDefaultDict(None,\n",
       "                         {'tokens': {'@@PADDING@@': 0,\n",
       "                           '@@UNKNOWN@@': 1,\n",
       "                           'I': 2,\n",
       "                           'do': 3,\n",
       "                           \"n't\": 4,\n",
       "                           'hate': 5,\n",
       "                           'notebooks': 6,\n",
       "                           ',': 7,\n",
       "                           'just': 8,\n",
       "                           'like': 9,\n",
       "                           'them': 10,\n",
       "                           '.': 11},\n",
       "                          'labels': {'technology': 0}})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab._token_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "serious-belief",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field._indexed_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "moving-fellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_field._label_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-twenty",
   "metadata": {},
   "source": [
    "Although we have constructed the mappings, we haven't yet used them to index the fields in our instance. We have to do that manually (although when you use the allennlp trainer all of this will be taken care of.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "another-thumbnail",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance.index_fields(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "conceptual-genome",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': {'tokens': [2, 3, 4, 5, 6, 7, 2, 8, 3, 4, 9, 10, 11]}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_field._indexed_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "covered-walter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_field._label_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "suitable-attack",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': {'tokens': {'tokens': tensor([ 2,  3,  4,  5,  6,  7,  2,  8,  3,  4,  9, 10, 11])}},\n",
       " 'category': tensor(0)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance.as_tensor_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "peaceful-potential",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': {'tokens___tokens': 13}, 'category': {}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance.get_padding_lengths()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-knife",
   "metadata": {},
   "source": [
    "# Batching and padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-indication",
   "metadata": {},
   "source": [
    "When you're doign NLP, you have sequences with different lengths, which means that padding and masking are very important. They're tricky to get right ! Luckily, AllenNLP handles most of the details for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "usual-michigan",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"I just don't like notebooks.\"\n",
    "tokens1 = tokenizer.tokenize(text)\n",
    "text_field1 = TextField(tokens1, {\"tokens\": token_indexer})\n",
    "label_field1 = LabelField(\"Joel\")\n",
    "instance1 = Instance({\"text\": text_field1, \"speaker\": label_field1})\n",
    "text2 = \"I do like notebooks.\"\n",
    "tokens2 = tokenizer.tokenize(text2)\n",
    "text_field2 = TextField(tokens2, {\"tokens\": token_indexer})\n",
    "label_field2 = LabelField(\"Tim\")\n",
    "instance2 = Instance({\"text\": text_field2, \"speaker\": label_field2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "practical-suggestion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9341c7b3ee224651ba58f91e62549559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='building vocab', max=2.0, style=ProgressStyle(descriptionâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from allennlp.data.batch import Batch\n",
    "vocab = Vocabulary.from_instances([instance1, instance2])\n",
    "\n",
    "batch = Batch([instance1, instance2])\n",
    "batch.index_instances(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "abandoned-surrey",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': {'tokens': {'tokens': tensor([[ 2,  3,  4,  8,  5,  9,  2, 10,  3,  4,  6, 11,  7],\n",
       "           [ 2,  3,  6,  5,  7,  0,  0,  0,  0,  0,  0,  0,  0]])}},\n",
       " 'speaker': tensor([0, 1])}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.as_tensor_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-designation",
   "metadata": {},
   "source": [
    "# Using Multiple Indexers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-polyester",
   "metadata": {},
   "source": [
    "In some circumstances, you might want to use multiple indexers. For instance, you might want to index a token using the token_id, but also a sequence of character_ids. This is as simple as adding extra token indexers to our text fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "palestinian-opportunity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.token_indexers import TokenCharactersIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "imported-outline",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_character_indexer = TokenCharactersIndexer(min_padding_length=3)\n",
    "\n",
    "text_field = TextField(tokens, {'tokens': token_indexer, 'token_characters': token_character_indexer})\n",
    "label_field = LabelField('technology')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "detailed-signal",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = Instance({'text': text_field, 'label': label_field})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "atlantic-valve",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e9ff4df59ac480582cce26ca75d37f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='building vocab', max=1.0, style=ProgressStyle(descriptionâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary.from_instances([instance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "constant-occupation",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance.index_fields(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "varying-medication",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': {'tokens': {'tokens': tensor([ 2,  3,  4,  5,  6,  7,  2,  8,  3,  4,  9, 10, 11])},\n",
       "  'token_characters': {'token_characters': tensor([[ 6,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [ 7,  3,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [ 5,  8,  2,  0,  0,  0,  0,  0,  0],\n",
       "           [ 9, 12,  2,  4,  0,  0,  0,  0,  0],\n",
       "           [ 5,  3,  2,  4, 13,  3,  3, 10, 11],\n",
       "           [14,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [ 6,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [15, 16, 11,  2,  0,  0,  0,  0,  0],\n",
       "           [ 7,  3,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [ 5,  8,  2,  0,  0,  0,  0,  0,  0],\n",
       "           [17, 18, 10,  4,  0,  0,  0,  0,  0],\n",
       "           [ 2,  9,  4, 19,  0,  0,  0,  0,  0],\n",
       "           [20,  0,  0,  0,  0,  0,  0,  0,  0]])}},\n",
       " 'label': tensor(0)}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance.as_tensor_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identified-gossip",
   "metadata": {},
   "source": [
    "# Token Embedders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-majority",
   "metadata": {},
   "source": [
    "Once w've our text represented as ids, we use token embedders to create tensor embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "informal-celtic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0af55145b624128a13fe9d5a55dcaeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='building vocab', max=2.0, style=ProgressStyle(descriptionâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text1 = \"I just don't like notebooks.\"\n",
    "tokens1 = tokenizer.tokenize(text)\n",
    "text_field1 = TextField(tokens1, {\"tokens\": token_indexer})\n",
    "label_field1 = LabelField(\"Joel\")\n",
    "instance1 = Instance({\"text\": text_field1, \"speaker\": label_field1})\n",
    "text2 = \"I do like notebooks.\"\n",
    "tokens2 = tokenizer.tokenize(text2)\n",
    "text_field2 = TextField(tokens2, {\"tokens\": token_indexer})\n",
    "label_field2 = LabelField(\"Tim\")\n",
    "instance2 = Instance({\"text\": text_field2, \"speaker\": label_field2})\n",
    "vocab = Vocabulary.from_instances([instance1, instance2])\n",
    "batch = Batch([instance1, instance2])\n",
    "batch.index_instances(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "compatible-appliance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': {'tokens': {'tokens': tensor([[ 2,  3,  4,  8,  5,  9,  2, 10,  3,  4,  6, 11,  7],\n",
       "           [ 2,  3,  6,  5,  7,  0,  0,  0,  0,  0,  0,  0,  0]])}},\n",
       " 'speaker': tensor([0, 1])}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_dict = batch.as_tensor_dict()\n",
    "tensor_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "seventh-processor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.token_embedders import Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "actual-concept",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = Embedding(num_embeddings=vocab.get_vocab_size(\"tokens\"), embedding_dim=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-eight",
   "metadata": {},
   "source": [
    "Accordingly, we can apply these embeddings to the indexed tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "black-north",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3082, -0.1054, -0.4880,  0.4342, -0.3927],\n",
       "         [-0.0926,  0.3000,  0.3102, -0.1149, -0.2023],\n",
       "         [-0.5468, -0.3153,  0.2503,  0.0751, -0.3173],\n",
       "         [-0.2189,  0.3179,  0.3703,  0.5374,  0.2767],\n",
       "         [-0.5462, -0.2733, -0.2118, -0.5640, -0.1482],\n",
       "         [-0.1358,  0.0344, -0.1710, -0.3339,  0.4811],\n",
       "         [ 0.3082, -0.1054, -0.4880,  0.4342, -0.3927],\n",
       "         [ 0.4478,  0.3009,  0.1941,  0.5378, -0.1945],\n",
       "         [-0.0926,  0.3000,  0.3102, -0.1149, -0.2023],\n",
       "         [-0.5468, -0.3153,  0.2503,  0.0751, -0.3173],\n",
       "         [-0.4426, -0.3893, -0.0188,  0.1502,  0.3023],\n",
       "         [-0.0896,  0.4304,  0.3974,  0.4833, -0.1077],\n",
       "         [ 0.1111, -0.4240, -0.3640,  0.5719, -0.2200]],\n",
       "\n",
       "        [[ 0.3082, -0.1054, -0.4880,  0.4342, -0.3927],\n",
       "         [-0.0926,  0.3000,  0.3102, -0.1149, -0.2023],\n",
       "         [-0.4426, -0.3893, -0.0188,  0.1502,  0.3023],\n",
       "         [-0.5462, -0.2733, -0.2118, -0.5640, -0.1482],\n",
       "         [ 0.1111, -0.4240, -0.3640,  0.5719, -0.2200],\n",
       "         [-0.0894, -0.2861,  0.4614, -0.2392, -0.1686],\n",
       "         [-0.0894, -0.2861,  0.4614, -0.2392, -0.1686],\n",
       "         [-0.0894, -0.2861,  0.4614, -0.2392, -0.1686],\n",
       "         [-0.0894, -0.2861,  0.4614, -0.2392, -0.1686],\n",
       "         [-0.0894, -0.2861,  0.4614, -0.2392, -0.1686],\n",
       "         [-0.0894, -0.2861,  0.4614, -0.2392, -0.1686],\n",
       "         [-0.0894, -0.2861,  0.4614, -0.2392, -0.1686],\n",
       "         [-0.0894, -0.2861,  0.4614, -0.2392, -0.1686]]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(tensor_dict['text']['tokens']['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-range",
   "metadata": {},
   "source": [
    "# Text Field Embedders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funded-activation",
   "metadata": {},
   "source": [
    "A Text Field emebdder may have multiple indexed representations of its tokens, in which case it needs multiple corresponding `TextFiedlEmbedders` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "funny-locator",
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "alternative-corpus",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field_embedder = BasicTextFieldEmbedder({\"tokens\": embedding})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "simple-assurance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3082, -0.1054, -0.4880,  0.4342, -0.3927],\n",
       "         [-0.0926,  0.3000,  0.3102, -0.1149, -0.2023],\n",
       "         [-0.5468, -0.3153,  0.2503,  0.0751, -0.3173],\n",
       "         [-0.2189,  0.3179,  0.3703,  0.5374,  0.2767],\n",
       "         [-0.5462, -0.2733, -0.2118, -0.5640, -0.1482],\n",
       "         [-0.1358,  0.0344, -0.1710, -0.3339,  0.4811],\n",
       "         [ 0.3082, -0.1054, -0.4880,  0.4342, -0.3927],\n",
       "         [ 0.4478,  0.3009,  0.1941,  0.5378, -0.1945],\n",
       "         [-0.0926,  0.3000,  0.3102, -0.1149, -0.2023],\n",
       "         [-0.5468, -0.3153,  0.2503,  0.0751, -0.3173],\n",
       "         [-0.4426, -0.3893, -0.0188,  0.1502,  0.3023],\n",
       "         [-0.0896,  0.4304,  0.3974,  0.4833, -0.1077],\n",
       "         [ 0.1111, -0.4240, -0.3640,  0.5719, -0.2200]],\n",
       "\n",
       "        [[ 0.3082, -0.1054, -0.4880,  0.4342, -0.3927],\n",
       "         [-0.0926,  0.3000,  0.3102, -0.1149, -0.2023],\n",
       "         [-0.4426, -0.3893, -0.0188,  0.1502,  0.3023],\n",
       "         [-0.5462, -0.2733, -0.2118, -0.5640, -0.1482],\n",
       "         [ 0.1111, -0.4240, -0.3640,  0.5719, -0.2200],\n",
       "         [-0.0894, -0.2861,  0.4614, -0.2392, -0.1686],\n",
       "         [-0.0894, -0.2861,  0.4614, -0.2392, -0.1686],\n",
       "         [-0.0894, -0.2861,  0.4614, -0.2392, -0.1686],\n",
       "         [-0.0894, -0.2861,  0.4614, -0.2392, -0.1686],\n",
       "         [-0.0894, -0.2861,  0.4614, -0.2392, -0.1686],\n",
       "         [-0.0894, -0.2861,  0.4614, -0.2392, -0.1686],\n",
       "         [-0.0894, -0.2861,  0.4614, -0.2392, -0.1686],\n",
       "         [-0.0894, -0.2861,  0.4614, -0.2392, -0.1686]]],\n",
       "       grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_field_embedder(tensor_dict['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-sympathy",
   "metadata": {},
   "source": [
    "# Seq2VecEncoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-notice",
   "metadata": {},
   "source": [
    "At this point, we've ended up with sequence of tensors. Frequently we'll want to collapse that sequence into a single contextualized tensor representations, which we do with `Seq2VecEncoders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "stylish-credit",
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.seq2vec_encoders import BagOfEmbeddingsEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "olympic-limit",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = BagOfEmbeddingsEncoder(embedding_dim=text_field_embedder.get_output_dim())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "specified-idaho",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5365, -0.2444,  0.3412,  2.1716, -1.4349],\n",
       "        [-1.3771, -3.1807,  2.9189, -1.4364, -2.0097]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(text_field_embedder(tensor_dict['text']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promotional-honolulu",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "acknowledged-poland",
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.models import Model\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder\n",
    "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder\n",
    "from typing import Dict\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "forty-pursuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(Model):\n",
    "    def __init__(self,\n",
    "                vocab: Vocabulary,\n",
    "                embedder: TextFieldEmbedder,\n",
    "                encoder: Seq2VecEncoder,\n",
    "                output_dim: int)->None:\n",
    "        super().__init__(vocab)\n",
    "        self.embedder = embedder\n",
    "        self.encoder = encoder\n",
    "        self.linear = torch.nn.Linear(in_features=embedder.get_output_dim(), out_features=output_dim)\n",
    "        \n",
    "    def forward(self, text: Dict[str, torch.Tensor], speaker:torch.Tensor)-> Dict[str, torch.Tensor]:\n",
    "        embedded = self.embedder(text)\n",
    "        encoded = self.encoder(embedded)\n",
    "        output = self.linear(encoded)\n",
    "        \n",
    "        return {'output': output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "going-disaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(vocab, text_field_embedder, encoder, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "brazilian-former",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': tensor([[-0.3698, -1.5098, -0.8451],\n",
       "         [-0.2740, -1.9789,  0.0265]], grad_fn=<AddmmBackward>)}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**tensor_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-burns",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
